Going forward gonna put notes here after every session. I've been using Claude Code far more than I originally intended. I'm unsure if this is actually a good thing or not. I feel pulled to use it partially because the rate it spits out code is more of a dopamine bust than writing by hand, but there are definetely actual benefits to using the LLM. It massively sped up putting in the framwork and when it works right it is at least 3 times as fast as me, if not the mythical 10x. There are two obvious downsides here. 1) I don't get to learn or practice as much which one of my original goals here. 2) Claude is not actually that good and when it makes mistakes it can take me much longer to find them because I'm less familiar with the codebase and the tech used(as a result of numnber one.)

This is somewhat offset by allowing practice prompting and learning claude's quirks. I may decide later to take over most of the coding manually but for now I'm switching to making claude the primary generator of code and sitting back into a project manager and the Tester role I'm more familiar with. We're getting to the point where it might be worth spinning up the api and testing by hand to see what jumps out. I'll probably do at least one more session looking to complete the fight service before I actually start down that path. 

General observations from before this session
1) Claude does not need to be babysat if given sufficient propmting. Can release the Opus model to just go write and check later. Using docker containers makes this somewhat safe, though I need to spend some time figure out the limits of that safety.
2) Sufficient prompting is incredibly tricky to test. Using a normal options, not in yolo mode, it can appear like the prompts have given claude sufficient guardrails, but when let off the leash, will often either have missed something important or claude's updating context will overwrite some area that wasn't specified enough.
3) As all the current guides are saying use of external files for claude to research appear to be key. I'm mostly only using a CLAUDE.MD, and a progress document at the moment for claude specific management. This will likely change as I go forward
4) Claude struggles with the incremental nature of TDD. Even with strict instructions to focus on a single test at a time claude will often go off and create all the tests or skip them entirely sometimes. This gets worse the longer the session, indicating context poisioning
5) In general there will be lots of small errors like confusing async and sync, mixing types(can't seem to get datetime with time stamp correct for me), and failure to properly use object or libraries. It can usually fix these with one or two more run throughs, but that gets expensive in token usage. Currently feels more efficient for me to debug those type of failurs rather than let claude do it. It may help to create a list of common errors like that in another reference file Claude can read at the begining of each session
6) Claude includes alot of metrics, many made up, and most not useful. I keep trying to remove references to total number  of tests as a criteria and claude keeps adding it back in.
7) Lots of time has been wasted trying to clean up documentation and tracking. Hoping to use Claude for project management has proven a diaster. It's possible that using a sperate agent for that would help. There is a popular model of having one agent control others, a simple version is a manager agent I communicate with, that then provides instructions to a coding agent and a seperate code reviewer agent. That does not feel tenable on the pro plan and I'm not ready to spend on the max or per token api for this project. Similary not worth trying the Ralph Wigguns for this. Probably something I experiment with later on another project.
8) I'm stuck thinking like a dotnet developer which can cause inconsistencies with Claudes more pythonic code. The question of whether I should take the time to learn the pythonic ways, try to include some prompting about using dotnet patterns/vocab, or ignore it and just have inconsistencies is one I'll need to address.
9) Claude is not actually good at creating or updating documentation as we go. Again perhaps best down via a sperate agent to keep context clean?
10) Claudes testing does not seem to adhere to either the one assert per test rule or a more assert all things you'd want tested strategy. I guess this makes sense if its the average of all unit test code avialable


Thoughts from this session

Did Much better at constraining Claude to TDD. Still wrote all the functional/BDD tests at once, but then implemented the code for them one at a time using red, green, red pattern and sticking to a single unit test at a time. I mentioned that multiple times in the first and subsequent prompts.

Had claude generate an updated CLAUDE.md and exited the session to see if that worked. Was able to give a simple get started prompt and followed the pattern. Used required actions, forbidden actions, a bolded rule, and a specific numbered list of steps to take in a loop. May play with this more but seems to work. 
Need to see how well this works with sonnet before I start messing though.

Claude still can't run the integration tests in docker. Will make that a priority for next session. I had to fix a failure to properly refresh sqlalchemy session and another mix up of naive and time zone aware date time. 

I'm also unclear what its doing with migrations. The CLI looks like its writing them itself, instead of generating them with a tool, which is how I would have expected it to work. 

Claude is also not making commits. Should try to update it so a new commit at min after every working test.


Ran 2 more small sessions trying Sonnet.
Managed to get Fight endpoints done, supposedly using business rules(I'm skeptical of this and need to confirm but that is gonna wait for a few days).
Sonnet did follow TDD succesfully so that was good. However it failed to follow the "BDD" flow I outlined of creating a functional integration test first and then implementing code to make it pass via TDD. It completely skipped integration tests. 

It also broke 7 unit tests and claimed they were already broken, so didn't fix them. Same with the one integration test that existed for fight. This has been a common pattern of skipping broken tests I'll need to address. Probably add a rule to always fix broken test. I can see 2 flaws that will come with that, one make the test fit the code not the other way around. And two getting stuck on a trivial problem when its better to move on. 

The broken unit tests were all a result of when I changed the return of getbyId in the fight repo to use result.unique().scalar_one_or_none() from just result.scalar_one_or_none(). This was a fix I got from googling without fully understaning. It seemed like relationship between fight and fightparticipation auto joins when queried. Something in there required a unique result, maybe because both tables have the relationship so it was being returned twice? Regardless the broken tests were instaniating magic_mock with just scalar_one_or_none and it needed to be unique().scalar_one_or_none(). I am worried this is not the right solution and I'm going to see the problem till it bites me in ass.

The broken integration test was the fight_data being passed into the controller was no longer being parsed right and was calling the service with incorrect parameters. Updated the controller to handle participations, format, and the service correctly. I do not think this is the right handling though. Format should be a tag and I'm not sure it's being handled like that. The JSON should look more like
{data:{
    fight:Date, url etc,
    team_one: team name, id, etc
    team_two: team name, id, etc
    Tags:{
        format:singles,
        category:null,
        etc
    }
    
}}

Looks like that's in decisions, which is not being referenced. I need to look into breaking CLAUDE.md up into more specific files that get referenced automatically when CLAUDE is working in an area related to that. The worry of course being if its a decision tree in a parent CLAUDE.md or something that it will get lost as context grow since CLAUDE is not deterministic. Possibly write an actual script to provide reminders to check proper files under specific conditions? 

need to do more research into how CC works and other options people have implemented.

Ended the session with Claude updating the PROGRESS.md to add a step to implement a CI/CD pipeline, so that Claude would be able to run integration tests while I still ran it in the sandboxed container. 